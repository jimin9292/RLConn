{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0410 15:17:43.494889 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:5: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "W0410 15:17:43.516357 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0410 15:17:43.523471 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:66: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0410 15:17:43.527235 4610170304 deprecation.py:323] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:68: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0410 15:17:44.035542 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:94: The name tf.squared_difference is deprecated. Please use tf.math.squared_difference instead.\n",
      "\n",
      "W0410 15:17:44.041154 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:96: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0410 15:17:44.351226 4610170304 deprecation_wrapper.py:119] From /Users/Frank/github/RLConn/RLConn/control_dqn.py:41: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "default_dir = os.path.dirname(os.getcwd())\n",
    "os.chdir(default_dir)\n",
    "\n",
    "import RLConn as rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observe(Gg, Gs):\n",
    "    N = Gg.shape[0]\n",
    "    return np.concatenate((Gg[np.triu_indices(N,k=1)],Gs[np.triu_indices(N,k=1)],Gs[np.tril_indices(N,k=-1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(Gg, Gs, action, location, dx = 0.1):\n",
    "    N = Gg.shape[0]\n",
    "    size = int(N*(N-1)/2)\n",
    "    change = dx*(action == 0) + (-dx)*(action == 2)\n",
    "    if location < size:\n",
    "        coordinate = np.array(np.triu_indices(N,k=1)).T[location]\n",
    "        Gg[coordinate[0], coordinate[1]] += change\n",
    "        Gg[coordinate[1], coordinate[0]] += change\n",
    "    elif location < 2*size:\n",
    "        coordinate = np.array(np.triu_indices(N,k=1)).T[location - size]\n",
    "        Gs[coordinate[0], coordinate[1]] += change\n",
    "    else: \n",
    "        coordinate = np.array(np.tril_indices(N,k=-1)).T[location - 2*size]\n",
    "        Gs[coordinate[0], coordinate[1]] += change\n",
    "    return Gg, Gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rand_action(N):\n",
    "    size = int(N*(N-1)/2)\n",
    "    return np.random.randint(2), np.random.randint(3*size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RLConn import problem_definitions as problems\n",
    "\n",
    "def get_reward(Gg, Gs):\n",
    "    with suppress_stdout():\n",
    "        mean_err,_ = rc.utils.compute_problem_score(Gg, Gs, problems.FOUR_NEURON_OSCILLATION, verbose=False)\n",
    "    np.random.seed() #reset seed\n",
    "    return -mean_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_x(x, Gg, Gs, reward):\n",
    "    obs = get_observe(Gg, Gs)\n",
    "    x = np.concatenate((x[len(obs)+1:], obs))\n",
    "    x = np.append(x,reward)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "N = 4\n",
    "size = int(3*N*(N-1)/2)\n",
    "PG = rc.alpg.ActLocPolicyGradient(\n",
    "    n_actions=3,\n",
    "    n_features=(size+1)*10)\n",
    "\n",
    "input_vec = np.zeros(4)\n",
    "input_vec[1] = 0.068\n",
    "ablation_mask = np.ones(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Frank/miniconda3/lib/python3.7/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/Users/Frank/github/RLConn/RLConn/network_sim.py:535: RuntimeWarning: overflow encountered in true_divide\n",
      "  J2 = np.multiply(Gsyn, J2_M4_2) / params_obj_neural['C']\n",
      "/Users/Frank/miniconda3/lib/python3.7/site-packages/scipy/integrate/_ode.py:1009: UserWarning: vode: Repeated convergence failures. (Perhaps bad Jacobian supplied or wrong choice of MF or tolerances.)\n",
      "  self.messages.get(istate, unexpected_istate_msg)))\n",
      "/Users/Frank/github/RLConn/RLConn/network_sim.py:515: RuntimeWarning: overflow encountered in true_divide\n",
      "  dV = (-(VsubEc + GapCon + SynapCon) + Input)/params_obj_neural['C']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   reward: -59395\n",
      "Gg: [  0 -14  13   0 -14   0   5 -11  13   5   0   1   0 -11   1   0]   Gs: [  0  13 -11  -1  10   0   5   0   1   2   0   8  13  13   6   0]\n",
      "400.5466386471142\n",
      "episode: 1   reward: -58855\n",
      "Gg: [ 0  9  6 13  9  0 14 11  6 14  0 11 13 11 11  0]   Gs: [ 0  8 18  1 10  0  1  8  5 14  0  4  6 13  5  0]\n",
      "17.883172803899498\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(3000):\n",
    "    #initialize x\n",
    "    x = np.array([])\n",
    "    network = rc.network_sim.generate_random_network(4, 1, 15)\n",
    "    Gg, Gs = network['gap'], network['syn']\n",
    "    reward = get_reward(Gg, Gs)\n",
    "    x = np.concatenate((x, get_observe(Gg, Gs)))\n",
    "    x = np.append(x,reward)\n",
    "    for i in range(9):\n",
    "        action, location = get_rand_action(N)\n",
    "        Gg, Gs = move(Gg, Gs, action, location, dx=1)\n",
    "        reward = get_reward(Gg, Gs)\n",
    "        x = np.concatenate((x, get_observe(Gg, Gs)))\n",
    "        x = np.append(x,reward)   \n",
    "        \n",
    "    #choose action using nn\n",
    "    for step in range(300):\n",
    "        action, location = PG.choose_action(x)\n",
    "        Gg, Gs = move(Gg, Gs, action, location, dx=1)\n",
    "        reward = get_reward(Gg, Gs)\n",
    "        PG.store_transition(x, action, location, reward)\n",
    "        x = update_x(x, Gg, Gs, reward)\n",
    "    ep_rs_sum = sum(PG.ep_rs)\n",
    "    if 'running_reward' not in globals():\n",
    "        running_reward = ep_rs_sum\n",
    "    else:\n",
    "        running_reward = running_reward * 0.99 + ep_rs_sum * 0.01\n",
    "    print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "    print(\"Gg:\", Gg.flatten(), \"  Gs:\", Gs.flatten())\n",
    "    with suppress_stdout():\n",
    "        mean_error, _ = rc.utils.compute_score(Gg, Gs, network['directionality'], \n",
    "                                                input_vec, ablation_mask,\n",
    "                                                  tf = 7, t_delta = 0.01,\n",
    "                                                  cutoff_1 = 100, cutoff_2 = 600,\n",
    "                                                  plot_result = True)\n",
    "    print(mean_error)\n",
    "    vt = PG.learn()\n",
    "    # print(vt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
